{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING AND LEMMATIZATION\n",
    "\n",
    "#### Demonstrating COCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming \n",
    "#Explore coca for three different search strings- include three personality each\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = stemmerporter.stem('sleeping')\n",
    "b = stemmerporter.stem('eating')\n",
    "c = stemmerporter.stem('craziness')\n",
    "d = stemmerporter.stem('laziness')\n",
    "e = stemmerporter.stem('dazzling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep\n",
      "eat\n",
      "crazi\n",
      "lazi\n",
      "dazzl\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmerLC = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = stemmerLC.stem('sleeping')\n",
    "b1 = stemmerLC.stem('eating')\n",
    "c1 = stemmerLC.stem('craziness')\n",
    "d1 = stemmerLC.stem('laziness')\n",
    "e1 = stemmerLC.stem('dazzling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep\n",
      "eat\n",
      "crazy\n",
      "lazy\n",
      "dazzl\n"
     ]
    }
   ],
   "source": [
    "print(a1)\n",
    "print(b1)\n",
    "print(c1)\n",
    "print(d1)\n",
    "print(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "st = RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = st.stem('sleeping')\n",
    "b0 = st.stem('eating')\n",
    "c0 = st.stem('craziness')\n",
    "d0 = st.stem('laziness')\n",
    "e0 = st.stem('dazzling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep\n",
      "eat\n",
      "crazines\n",
      "lazines\n",
      "dazzl\n"
     ]
    }
   ],
   "source": [
    "print(a0)\n",
    "print(b0)\n",
    "print(c0)\n",
    "print(d0)\n",
    "print(e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['पूर्ण', 'प्रतिबंध', 'हटाओ', ':', 'इराक', 'संयुक्त', ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.corpus.indian.words('hindi.pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate snowball stemming in HIndi/Tamil/French\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bonjour'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "French_stemmer = SnowballStemmer('french')\n",
    "French_stemmer.stem ('Bonjoura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An quick brown fox jump over a lazi dog\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "ex = \"An quick brown fox jumped over a lazy dog\"\n",
    "ex = [stemmer.stem(token) for token in ex.split(\" \")]\n",
    "print(\" \".join(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Lemmatization?\n",
    "Lemmatization technique is like stemming. The output we get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word.\n",
    "\n",
    "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand it with an example −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lemmatizer\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = lemmatizer.lemmatize('eating')\n",
    "w1 = lemmatizer.lemmatize('books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating\n",
      "book\n"
     ]
    }
   ],
   "source": [
    "print(q1)\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   RE stemmer          Lemmatizer          \n",
      "friend              friend              friend              friend              friend              \n",
      "friendship          friendship          friend              friendship          friendship          \n",
      "friends             friend              friend              friend              friend              \n",
      "friendships         friendship          friend              friendship          friendship          \n",
      "stabil              stabil              stabl               stabil              stabil              \n",
      "destabilize         destabil            dest                destabiliz          destabilize         \n",
      "misunderstanding    misunderstand       misunderstand       misunderstand       misunderstanding    \n",
      "railroad            railroad            railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             football            football            \n",
      "sleeping            sleep               sleep               sleep               sleeping            \n",
      "eating              eat                 eat                 eat                 eating              \n",
      "walking             walk                walk                walk                walking             \n",
      "drinking            drink               drink               drink               drinking            \n",
      "staring             stare               star                star                staring             \n",
      "glancing            glanc               glant               glanc               glancing            \n",
      "steering            steer               ste                 steer               steering            \n",
      "handling            handl               handl               handl               handling            \n",
      "estimation          estim               estim               estimation          estimation          \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\",\"sleeping\",\"eating\",\"walking\",\"drinking\",\"staring\",\"glancing\",\"steering\",\"handling\",\"estimation\"]\n",
    "print(\"{0:20}{1:20}{2:20}{3:20}{4:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\",\"RE stemmer\", \"Lemmatizer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:20}{4:20}\".format(word,stemmerporter.stem(word),stemmerLC.stem(word),st.stem(word),lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全 模式 ， 把 句子 中所 所有 的 可以 成 词 的 词语 都 扫描 描出 描出来 出来 ,    速度 非常 快 ， 但是 不能 能解 解决 歧义\n"
     ]
    }
   ],
   "source": [
    "#chinese segmentation using JIEBA\n",
    "import jieba\n",
    "seg = jieba.cut(\"全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义\", cut_all = True)\n",
    "print(\" \".join((seg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['支持三种分词模式：精确模式，试图将句子最精确地切开，适合文本分析；全模式，']\n"
     ]
    }
   ],
   "source": [
    "#Basic text procssing pipelining\n",
    "import nltk\n",
    "sent = \"支持三种分词模式：精确模式，试图将句子最精确地切开，适合文本分析；全模式，\"\n",
    "        \n",
    "words = nltk.word_tokenize(sent)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lemmatization', 'is', 'the', 'process', 'of', 'grouping', 'together', 'the', 'different', 'inflected', 'forms', 'of', 'a', 'word', 'so', 'they', 'can', 'be', 'analysed', 'as', 'a', 'single', 'item', '.']\n",
      "['Lemmatization', 'is', 'similar', 'to', 'stemming', 'but', 'it', 'brings', 'context', 'to', 'the', 'words', '.']\n",
      "['So', 'it', 'links', 'words', 'with', 'similar', 'meaning', 'to', 'one', 'word', '.']\n",
      "['Text', 'preprocessing', 'includes', 'both', 'Stemming', 'as', 'well', 'as', 'Lemmatization', '.']\n",
      "['Many', 'times', 'people', 'find', 'these', 'two', 'terms', 'confusing', '.']\n",
      "['Some', 'treat', 'these', 'two', 'as', 'same', '.']\n",
      "['Actually', ',', 'lemmatization', 'is', 'preferred', 'over', 'Stemming', 'because', 'lemmatization', 'does', 'morphological', 'analysis', 'of', 'the', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "text = [\"Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word. Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\"]\n",
    "import nltk\n",
    "for texts in text:\n",
    "    sentences = nltk.sent_tokenize(texts)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thanjavur', ',', 'formerly', 'Tanjore', ',', 'is', 'a', 'city', 'in', 'the', 'Indian', 'state', 'of', 'Tamil', 'Nadu', '.']\n",
      "['Thanjavur', 'is', 'an', 'important', 'center', 'of', 'South', 'Indian', 'religion', ',', 'art', ',', 'and', 'architecture', '.']\n",
      "['Most', 'of', 'the', 'Great', 'Living', 'Chola', 'Temples', ',', 'which', 'are', 'UNESCO', 'World', 'Heritage', 'Monuments', ',', 'are', 'located', 'in', 'and', 'around', 'Thanjavur', '.']\n",
      "['The', 'foremost', 'among', 'these', ',', 'the', 'Brihadeeswara', 'Temple', ',', 'is', 'located', 'in', 'the', 'centre', 'of', 'the', 'city', '.']\n",
      "['Thanjavur', 'is', 'also', 'home', 'to', 'Tanjore', 'painting', ',', 'a', 'painting', 'style', 'unique', 'to', 'the', 'region', '.']\n",
      "['Thanjavur', 'is', 'the', 'headquarters', 'of', 'the', 'Thanjavur', 'District', '.']\n",
      "['The', 'city', 'is', 'an', 'important', 'agricultural', 'centre', 'located', 'in', 'the', 'Cauvery', 'Delta', 'and', 'is', 'known', 'as', 'the', 'Rice', 'bowl', 'of', 'Tamil', 'Nadu', '.']\n",
      "['Thanjavur', 'is', 'administered', 'by', 'a', 'municipal', 'corporation', 'covering', 'an', 'area', 'of', '36.33', 'km2', '(', '14.03', 'sq', 'mi', ')', 'and', 'had', 'a', 'population', 'of', '222,943', 'in', '2011', '.']\n",
      "['Roadways', 'are', 'the', 'major', 'means', 'of', 'transportation', ',', 'while', 'the', 'city', 'also', 'has', 'rail', 'connectivity', '.']\n",
      "['The', 'nearest', 'airport', 'is', 'Tiruchirapalli', 'International', 'Airport', ',', 'located', '59.6', 'km', '(', '37.0', 'mi', ')', 'away', 'from', 'the', 'city', '.']\n",
      "['The', 'nearest', 'seaport', 'is', 'Karaikal', ',', 'which', 'is', '94', 'km', '(', '58', 'mi', ')', 'away', 'from', 'Thanjavur', '.']\n"
     ]
    }
   ],
   "source": [
    "text1 = [\"Thanjavur, formerly Tanjore, is a city in the Indian state of Tamil Nadu. Thanjavur is an important center of South Indian religion, art, and architecture. Most of the Great Living Chola Temples, which are UNESCO World Heritage Monuments, are located in and around Thanjavur. The foremost among these, the Brihadeeswara Temple, is located in the centre of the city. Thanjavur is also home to Tanjore painting, a painting style unique to the region. Thanjavur is the headquarters of the Thanjavur District. The city is an important agricultural centre located in the Cauvery Delta and is known as the Rice bowl of Tamil Nadu. Thanjavur is administered by a municipal corporation covering an area of 36.33 km2 (14.03 sq mi) and had a population of 222,943 in 2011. Roadways are the major means of transportation, while the city also has rail connectivity. The nearest airport is Tiruchirapalli International Airport, located 59.6 km (37.0 mi) away from the city. The nearest seaport is Karaikal, which is 94 km (58 mi) away from Thanjavur.\"] \n",
    "for texts in text1:\n",
    "    sentences = nltk.sent_tokenize(texts)\n",
    "    for sentence in sentences:\n",
    "        words1 = nltk.word_tokenize(sentence)\n",
    "        print(words1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
